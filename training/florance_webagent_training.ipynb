{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nVgIlFxJs4a"
      },
      "source": [
        "# Introduction\n",
        "This notebook provides a comprehensive guide to fine-tuning Florance-2 with a custom task called **`<WebAgent>`**, designed to detect and identify webpage elements based on natural language descriptions. The goal of this fine-tuning process is to enhance the modelâ€™s ability to recognize UI components, such as buttons, links, and images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFzLZnvMhG1u"
      },
      "source": [
        "# Install required dependencies\n",
        "Before running the next steps, install the necessary dependencies:\n",
        "\n",
        "* **matplotlib** for visualization\n",
        "\n",
        "* **transformers** for model loading and fine-tuning\n",
        "\n",
        "* **datasets** for loading training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlQ8zVhTd9G5"
      },
      "outputs": [],
      "source": [
        "!pip install -q matplotlib\n",
        "!pip install -q transformers\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ktUHI3Kdm5r"
      },
      "source": [
        "# Model\n",
        "We will use Florance 2 for creating the base of our webagent. The model should be able to detect any item on a webpage given a description. For example, \"A button that contains the text 'Login'\"\n",
        "\n",
        "\n",
        "> Read more about Florance 2 [here](https://www.microsoft.com/en-us/research/publication/florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/)\n",
        "\n",
        "\n",
        "\n",
        "We will be using the florance model from hugging face for fine tuning on our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I6_GGEWeO_k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = 'microsoft/Florence-2-base-ft'\n",
        "custom_task = \"<WebAgent>\"\n",
        "\n",
        "def load_model(model_path=None):\n",
        "   \"\"\"\n",
        "    Loads the Florence model and processor.\n",
        "\n",
        "    Args:\n",
        "        model_path (str, optional): Path to a locally saved model. If None, loads from Hugging Face.\n",
        "\n",
        "    Returns:\n",
        "        processor (AutoProcessor): Pre-trained processor for handling text and images.\n",
        "        model (AutoModelForCausalLM): Loaded model ready for inference or fine-tuning.\n",
        "    \"\"\"\n",
        "    if(model_path == None):\n",
        "      model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      trust_remote_code=True,\n",
        "      ).to(device)\n",
        "      processor = AutoProcessor.from_pretrained(model_name,\n",
        "                                                trust_remote_code=True)\n",
        "\n",
        "      return processor, model\n",
        "\n",
        "    else:\n",
        "      model = AutoModelForCausalLM.from_pretrained(model_path,\n",
        "                                                   trust_remote_code=True,).to(device)\n",
        "      processor = AutoProcessor.from_pretrained(model_path,\n",
        "                                                trust_remote_code=True)\n",
        "\n",
        "      return processor, model\n",
        "\n",
        "processor, model = load_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcHak8GidHKa"
      },
      "source": [
        "## Processor modifications\n",
        "Refer to https://huggingface.co/microsoft/Florence-2-large/blob/main/processing_florence2.py for the processor implementation.\n",
        "\n",
        "We will add a custom task and train the model on that task. We will modify the following variables in the processor which will allow us to introduce our custom task to the existing processor logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee0GAJ0RdJ8k"
      },
      "outputs": [],
      "source": [
        "processor.tasks_answer_post_processing_type[custom_task] = 'description_with_bboxes_or_polygons'\n",
        "processor.task_prompts_with_input[custom_task] = 'Locate {input} in the image.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnxwh_Z1X3ln"
      },
      "source": [
        "## Helper functions\n",
        "Below are the functions which will be used to handle model related tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVJRPLIZIVuv"
      },
      "source": [
        "### Plotting bouding boxes and polygons around the detected element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJzS_IadX3T2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_bbox_and_polygon(image, data):\n",
        "    \"\"\"\n",
        "    Plots bounding boxes and polygons on an image.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): Image to be annotated.\n",
        "        data (dict): Dictionary containing bounding boxes, labels, and polygon information.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "    ax.imshow(image)\n",
        "    for bbox, bbox_label, in zip(data['bboxes'], data['bboxes_labels']):\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        rect = patches.Rectangle((x1, y1),\n",
        "                                 x2 - x1,\n",
        "                                 y2 - y1,\n",
        "                                 linewidth=2,\n",
        "                                 edgecolor='lime',\n",
        "                                 facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(x1,\n",
        "                 y1,\n",
        "                 bbox_label,\n",
        "                 color='black',\n",
        "                 fontsize=8,\n",
        "                 bbox=dict(facecolor='lime', alpha=1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for polygons, polygon_label, in zip(data['polygons'], data['polygons_labels']):\n",
        "      for _polygon in polygons:\n",
        "              polygon = np.array(_polygon).reshape(-1, 2)  # Convert list of points to numpy array\n",
        "              ax.plot(polygon[:, 0], polygon[:, 1], marker='o', linestyle='-', color='red', linewidth=2)\n",
        "              ax.fill(polygon[:, 0], polygon[:, 1], color='red', alpha=0.3)  # Fill polygon with transparency\n",
        "\n",
        "              # Label polygon (placing the text at the first point of the polygon)\n",
        "              plt.text(polygon[0, 0], polygon[0, 1], polygon_label, color='white', fontsize=8,\n",
        "                      bbox=dict(facecolor='red', alpha=0.8))\n",
        "\n",
        "\n",
        "    ax.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Krn3pZIM3q"
      },
      "source": [
        "### Output helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhlJ8oyKa8Iu"
      },
      "outputs": [],
      "source": [
        "def get_model_output(model, processor, images, task_prompt, text_input=None):\n",
        "    \"\"\"\n",
        "    Generates model output for a given image and task prompt.\n",
        "\n",
        "    Args:\n",
        "        model (AutoModelForCausalLM): The fine-tuned Florence model.\n",
        "        processor (AutoProcessor): The processor to handle inputs.\n",
        "        images (PIL.Image): Input image.\n",
        "        task_prompt (str): Task-specific prompt.\n",
        "        text_input (str, optional): Additional text input to refine the prompt.\n",
        "\n",
        "    Returns:\n",
        "        dict: Processed response containing detected bounding boxes and polygons.\n",
        "    \"\"\"\n",
        "    if text_input is None:\n",
        "        prompt = task_prompt\n",
        "    else:\n",
        "        prompt = task_prompt + text_input\n",
        "\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        early_stopping=False,\n",
        "        do_sample=False,\n",
        "        num_beams=3,\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids,\n",
        "                                            skip_special_tokens=False)[0]\n",
        "\n",
        "    print(generated_text)\n",
        "\n",
        "    parsed_answer = processor.post_process_generation(\n",
        "          generated_text,\n",
        "          task=task_prompt,\n",
        "          image_size=(image.width, image.height))\n",
        "\n",
        "    return parsed_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At2nUcIlgMZY"
      },
      "source": [
        "### Bounding box helper functions.\n",
        "Florance requires bouding boxes to be formatted to 'loc_x', 'loc_y' The following methods will convert a bounding boxes into the required format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwNVVo8fgAjl"
      },
      "outputs": [],
      "source": [
        "def convert_bbox_to_relative(box, image):\n",
        "  \"\"\" converts bounding box pixel coordinates to relative coordinates in the\n",
        "  range 0-999 \"\"\"\n",
        "  return [\n",
        "      round((box[0]/image.width)*999),\n",
        "      round((box[1]/image.height)*999),\n",
        "      round((box[2]/image.width)*999),\n",
        "      round((box[3]/image.height)*999),\n",
        "    ]\n",
        "\n",
        "def convert_relative_to_loc(relative_coordinates):\n",
        "  \"\"\" converts a list of relative coordinate positions x1, y1, x2, y2 to a\n",
        "  string of position tokens \"\"\"\n",
        "  return ''.join([f'<loc_{i}>' for i in relative_coordinates])\n",
        "\n",
        "def convert_bbox_to_loc(box, image):\n",
        "  \"\"\" convert bounding box pixel coordinates to position tokens \"\"\"\n",
        "  relative_coordinates = convert_bbox_to_relative(box, image)\n",
        "  return convert_relative_to_loc(relative_coordinates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6FEhYaDOly-"
      },
      "outputs": [],
      "source": [
        "# helper function to render a few examples from the validation dataset during training\n",
        "def render_inference_results(model, processor, dataset, count, task):\n",
        "    html_out = \"\"\n",
        "    count = min(count, len(dataset))\n",
        "    for i in range(count):\n",
        "        prefix, suffix, image = dataset.__getitem__(i)\n",
        "        modified_prefix = prefix.replace(task, \"\")\n",
        "        response = get_model_output(model,processor,image, task,modified_prefix)\n",
        "        plot_bbox_and_polygon(image,response[task])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvDtA_2pUD09"
      },
      "source": [
        "## Run the base model\n",
        "Here we will try out the model with a **`WebAgent`** task that we have introduced into the processor. We will draw a bounding box around the detected object using our helper functions defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGjzgNEBUCMU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "response = requests.get(\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/20210401151214/What-is-Website.png\")\n",
        "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "text_prompt = \"Arrows\"\n",
        "response = get_model_output(model,processor,image,custom_task,text_prompt)\n",
        "\n",
        "print(\"Processed Response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypW_KGu7ZO8e"
      },
      "outputs": [],
      "source": [
        "plot_bbox_and_polygon(image,response[custom_task])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it8n1DTeaIXI"
      },
      "source": [
        "# Dataset\n",
        "We will uset the wave-ui-25k dataset to train our model to detect different elements from a web-page.\n",
        "\n",
        "**wave-ui-25k** - https://huggingface.co/datasets/agentsea/wave-ui-25k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0spBqIreljp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"agentsea/wave-ui-25k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TsFEuetiVAi"
      },
      "outputs": [],
      "source": [
        "# since the split only has train, we will manually split it into train,valid and test\n",
        "\n",
        "# First, split into 80% train and 20% temp (validation + test)\n",
        "temp_split = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Now, split temp into 50% validation, 50% test (resulting in 10% each)\n",
        "final_split = temp_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Assign datasets\n",
        "train_split = temp_split[\"train\"]\n",
        "valid_split = final_split[\"train\"]\n",
        "test_split = final_split[\"test\"]\n",
        "\n",
        "# Print sizes\n",
        "print(f\"Train size: {len(train_split)}, Validation size: {len(valid_split)}, Test size: {len(test_split)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k2yERyjTB7T"
      },
      "source": [
        "## Create the custom dataset for training\n",
        "Here we will create a custom dataset that returns a prefix, suffix and the image.\n",
        "* **The prefix** contains the custom task label followed by the prompt (description of the element we are trying to find in the page)\n",
        "* **The suffix** is what the model should output, i.e the bounding boxes or a polygon.\n",
        "If you want to return polygons, you might want to wrap the suffix with a `<poly>` tag. The format would look like - `label<poly><loc_x><loc_y>...</poly>`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01Ctl_g3TA3G"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WebUI(Dataset):\n",
        "\n",
        "    def __init__(self,dataset):\n",
        "        self.data = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        description = item['description']\n",
        "        image = item['image']\n",
        "\n",
        "        prefix = custom_task + description\n",
        "        suffix = description + convert_bbox_to_loc(item['bbox'],item['image'])\n",
        "        image = item['image'].convert(\"RGB\")\n",
        "        return prefix, suffix, image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzn4b1nW5BL6"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_cDLiOgNc22"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 4 # adjust the batch size based on your gpu memory\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "def collate_fn(batch):\n",
        "    prefix, suffix, images = zip(*batch)\n",
        "    inputs = processor(text=list(prefix), images=list(images),\n",
        "                       return_tensors=\"pt\", padding=True).to(device)\n",
        "    return inputs, suffix\n",
        "\n",
        "train_dataset = WebUI(train_split)\n",
        "valid_dataset = WebUI(valid_split)\n",
        "test_dataset = WebUI(test_split)\n",
        "\n",
        "# Slice the dataset to only include the first 1000 samples\n",
        "# train_subset = torch.utils.data.Subset(train_dataset, range(1000))\n",
        "\n",
        "# To test the training with a subset of the data, uncomment the above line\n",
        "# and replace train_dataset below with train_subset.\n",
        "# This will allow you to ensure your training is running as intended\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                          collate_fn=collate_fn, num_workers=NUM_WORKERS,\n",
        "                          shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
        "                          collate_fn=collate_fn, num_workers=NUM_WORKERS,\n",
        "                          shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D08IyZ5U7UvW"
      },
      "source": [
        "## LoRA config\n",
        "We will fine tune our model using LoRA. Since the original model is pretty good at OCR and finding objects from an image, we will try to keep as much knowledge of the original model as possible. Hence **we use small values for r and alpha** ensuring our fine tuned model does not deviate too much from its original state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uaAJAcEN9uy"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "TARGET_MODULES = [\n",
        "    \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
        "    \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"\n",
        "]\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=4, # adjust as required\n",
        "    lora_alpha=8, # adjust as required\n",
        "    target_modules=TARGET_MODULES,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    inference_mode=False,\n",
        "    use_rslora=True,\n",
        "    init_lora_weights=\"gaussian\",\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqRbn3zy8Evb"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfMUi9H979KD"
      },
      "source": [
        "## Traning Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXdFKS2B8HRi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_scheduler\n",
        ")\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
        "    \"\"\"\n",
        "    Trains the Florence model using LoRA fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        train_loader (DataLoader): Training dataset loader.\n",
        "        val_loader (DataLoader): Validation dataset loader.\n",
        "        model (AutoModelForCausalLM): The model to be trained.\n",
        "        processor (AutoProcessor): The processor for handling input data.\n",
        "        epochs (int, optional): Number of training epochs. Default is 5.\n",
        "        lr (float, optional): Learning rate for optimization. Default is 5e-6.\n",
        "    \"\"\"\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    num_training_steps = epochs * len(train_loader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            pixel_values = inputs[\"pixel_values\"]\n",
        "            labels = processor.tokenizer(\n",
        "                text=answers,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                return_token_type_ids=False\n",
        "            ).input_ids.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
        "\n",
        "                input_ids = inputs[\"input_ids\"]\n",
        "                pixel_values = inputs[\"pixel_values\"]\n",
        "                labels = processor.tokenizer(\n",
        "                    text=answers,\n",
        "                    return_tensors=\"pt\",\n",
        "                    # max_length=1024,\n",
        "                    # truncation=True,\n",
        "                    padding=True,\n",
        "                    return_token_type_ids=False\n",
        "                ).input_ids.to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Average Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "            # comment out the line below to avoid displaying images after every epoch\n",
        "            render_inference_results(model, processor, valid_dataset, 4, custom_task)\n",
        "\n",
        "        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        processor.save_pretrained(output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIWbfJ7H9I7f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "EPOCHS = 5 # adjust as required\n",
        "LR = 5e-6\n",
        "\n",
        "train_model(train_loader, valid_loader, peft_model, processor, epochs=EPOCHS, lr=LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYpAEmCXI5v-"
      },
      "source": [
        "## Download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XI8q3y_p0_5"
      },
      "outputs": [],
      "source": [
        "!zip -r model_checkpoint.zip ./model_checkpoints/epoch_5 # change as required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4D1nfH2ocDc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('model_checkpoint.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sym_fYHlrGG4"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTkvJrMXJQkt"
      },
      "source": [
        "## Test manually on a webpage screenshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdEyi3njZ4Jc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "peft_model.eval()\n",
        "text_prompt = \"Back button\"\n",
        "response = requests.get(\"https://media.gcflearnfree.org/content/55e07807bae0135431cfdcb3_12_17_2013/read_webpage_labeled.jpg\")\n",
        "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "response = get_model_output(peft_model,processor,image,custom_task,text_prompt)\n",
        "\n",
        "print(\"Processed Response:\", response)\n",
        "plot_bbox_and_polygon(image,response[custom_task])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
